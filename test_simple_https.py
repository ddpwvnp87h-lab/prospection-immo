#!/usr/bin/env python3
"""
Test simple HTTPS scraping - Sans dÃ©pendances externes!
Utilise uniquement la bibliothÃ¨que standard Python
"""

import urllib.request
import json
from html.parser import HTMLParser
from datetime import datetime


class SimpleHTMLParser(HTMLParser):
    """Parser HTML simple pour extraire les donnÃ©es."""

    def __init__(self):
        super().__init__()
        self.in_price = False
        self.in_title = False
        self.prices = []
        self.titles = []
        self.links = []

    def handle_starttag(self, tag, attrs):
        attrs_dict = dict(attrs)

        # DÃ©tecter les prix (contiennent souvent 'price' dans la classe)
        if 'class' in attrs_dict:
            class_name = attrs_dict['class'].lower()
            if 'price' in class_name or 'prix' in class_name:
                self.in_price = True
            if 'title' in class_name or 'titre' in class_name:
                self.in_title = True

        # Collecter les liens
        if tag == 'a' and 'href' in attrs_dict:
            self.links.append(attrs_dict['href'])

    def handle_data(self, data):
        data = data.strip()
        if self.in_price and data:
            self.prices.append(data)
        if self.in_title and data:
            self.titles.append(data)

    def handle_endtag(self, tag):
        self.in_price = False
        self.in_title = False


def test_https_request():
    """Teste une requÃªte HTTPS rÃ©elle."""

    print("="*80)
    print("ğŸŒ TEST HTTPS - Scraping avec bibliothÃ¨que standard Python")
    print("="*80)
    print()

    # URL de test - un site simple d'exemple
    test_urls = [
        "https://httpbin.org/html",  # Site de test HTTP
        "https://example.com",  # Site basique
    ]

    for url in test_urls:
        print(f"ğŸ“¡ Test #{test_urls.index(url) + 1}: {url}")
        print("-" * 80)

        try:
            # 1. CrÃ©er la requÃªte avec headers
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            req = urllib.request.Request(url, headers=headers)

            # 2. Envoyer la requÃªte HTTPS
            print("  ğŸ” Connexion HTTPS...")
            with urllib.request.urlopen(req, timeout=10) as response:
                # 3. Lire la rÃ©ponse
                print(f"  âœ… Status Code: {response.status}")
                print(f"  âœ… Content-Type: {response.headers.get('Content-Type', 'N/A')}")

                html_content = response.read().decode('utf-8')
                print(f"  âœ… Taille: {len(html_content):,} bytes")

                # 4. Parser le HTML
                parser = SimpleHTMLParser()
                parser.feed(html_content)

                # 5. Afficher les rÃ©sultats
                print(f"\n  ğŸ“Š DonnÃ©es extraites:")
                print(f"    â€¢ Liens trouvÃ©s: {len(parser.links)}")
                if parser.links[:3]:
                    for i, link in enumerate(parser.links[:3], 1):
                        print(f"      {i}. {link[:70]}")

                print(f"    â€¢ Titres trouvÃ©s: {len(parser.titles)}")
                if parser.titles[:3]:
                    for i, title in enumerate(parser.titles[:3], 1):
                        print(f"      {i}. {title[:70]}")

                print(f"    â€¢ Prix trouvÃ©s: {len(parser.prices)}")
                if parser.prices[:3]:
                    for i, price in enumerate(parser.prices[:3], 1):
                        print(f"      {i}. {price[:70]}")

        except Exception as e:
            print(f"  âŒ Erreur: {e}")

        print()


def demo_site_immobilier():
    """Montre comment Ã§a marche pour un site immobilier."""

    print("="*80)
    print("ğŸ  DÃ‰MO: Structure d'un scraper immobilier")
    print("="*80)
    print()

    print("ğŸ“ Workflow typique:")
    print("-" * 80)

    workflow = [
        ("1. URL de recherche", "https://www.pap.fr/annonce/vente-appartement-paris-75"),
        ("2. RequÃªte HTTPS GET", "urllib.request.urlopen(url)"),
        ("3. RÃ©cupÃ©ration HTML", "html = response.read().decode('utf-8')"),
        ("4. Parsing", "parser.feed(html)"),
        ("5. Extraction", "Trouve les divs avec class='annonce'"),
        ("6. Normalisation", "CrÃ©e un dict avec titre, prix, lien, etc."),
    ]

    for step, detail in workflow:
        print(f"  {step}")
        print(f"    â†’ {detail}")

    print()
    print("ğŸ“¦ Exemple de donnÃ©es extraites:")
    print("-" * 80)

    example_listing = {
        "titre": "Appartement 3 piÃ¨ces - 75mÂ² - Paris 15Ã¨me",
        "date_publication": datetime.now().strftime('%Y-%m-%d'),
        "prix": 450000,
        "localisation": "Paris 15Ã¨me",
        "lien": "https://www.pap.fr/annonce/exemple-123456",
        "site_source": "pap.fr",
        "photos": [
            "https://www.pap.fr/photos/exemple1.jpg"
        ],
        "telephone": "Non visible",
        "surface": 75,
        "pieces": 3,
        "description": "Bel appartement lumineux..."
    }

    print(json.dumps(example_listing, indent=2, ensure_ascii=False))
    print()


def show_real_code():
    """Montre le code rÃ©el utilisÃ© dans les scrapers."""

    print("="*80)
    print("ğŸ’» CODE RÃ‰EL: Extrait de scrapers/pap.py")
    print("="*80)
    print()

    code = '''
# Dans scrapers/pap.py (lignes 35-60)

session = requests.Session()
session.headers.update({'User-Agent': self.user_agent})

# URL de recherche
url = f"https://www.pap.fr/annonce/vente-immobilier-{ville}"

# RequÃªte HTTPS
response = session.get(url, timeout=15)
response.raise_for_status()

# Parsing HTML
soup = BeautifulSoup(response.content, 'lxml')

# Trouver les annonces
ads = soup.find_all('div', class_='search-list-item')

for ad in ads:
    # Extraire le titre
    titre_elem = ad.find('span', class_='item-title')
    titre = titre_elem.get_text(strip=True)

    # Extraire le prix
    prix_elem = ad.find('span', class_='item-price')
    prix = self._parse_price(prix_elem.get_text())

    # Extraire le lien
    link_elem = ad.find('a', href=True)
    lien = f"https://www.pap.fr{link_elem['href']}"

    # CrÃ©er l'objet annonce
    listing = {
        'titre': titre,
        'prix': prix,
        'lien': lien,
        ...
    }
'''

    print(code)
    print()


def main():
    """Point d'entrÃ©e."""

    print("\n" + "ğŸ¬ " * 20)
    print("DÃ‰MONSTRATION: HTTPS Scraping en Python")
    print("ğŸ¬ " * 20 + "\n")

    # Test 1: RequÃªte HTTPS basique
    test_https_request()

    # Test 2: Workflow immobilier
    demo_site_immobilier()

    # Test 3: Code rÃ©el
    show_real_code()

    print("="*80)
    print("ğŸ¯ CONCLUSION")
    print("="*80)
    print()
    print("âœ… HTTPS fonctionne avec urllib (bibliothÃ¨que standard)")
    print("âœ… Pour de vrais sites, utiliser requests + BeautifulSoup")
    print("âœ… Les scrapers dans le projet utilisent:")
    print("   â€¢ requests pour HTTPS")
    print("   â€¢ BeautifulSoup pour parser HTML")
    print("   â€¢ Playwright pour JavaScript dynamique")
    print()
    print("ğŸ’¡ Pour installer et tester:")
    print("   pip3 install requests beautifulsoup4")
    print("   python3 test_scrapers.py --ville Paris --site pap")
    print()


if __name__ == '__main__':
    main()
